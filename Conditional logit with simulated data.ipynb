{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating conditional logit function to Python\n",
    "### Author: Lauri Kyt√∂maa\n",
    "### Date created: January, 22, 2019\n",
    "\n",
    "### This Jupyter notebook translates Aguirregabiria and Mira's Gauss code named \"clogit.src\" into Python. This code estimates McFadden's Conditional Logit model using Newton's method with analytical gradient and the hessian.\n",
    "\n",
    "The original Gauss file can be found in the original author's Nested Pseudo Likelihood (NPL) zip file",
    "/n",
    "* **AM code link**:\n",
    "http://individual.utoronto.ca/vaguirre/wpapers/program_code_survey_joe_2008.html\n",
    "* **McFadden theory:** https://eml.berkeley.edu/reprints/mcfadden/zarembka.pdf, https://epubs.siam.org/doi/pdf/10.1137/0908006\n",
    "\n",
    "-------------\n",
    "\n",
    "## Parametric set-up\n",
    "\n",
    "Individual $i$'s latent utility for alternative $j \\in \\{1,\\dots,J\\}$ is given by: \n",
    "\\begin{equation*}\n",
    "y_{ij} = z_{ij1}\\beta_1 +z_{ij2}\\beta_2 + z_{ij3}\\beta_3 + r_{ij} + \\varepsilon_{ij}\n",
    "\\end{equation*}\n",
    "Where:\n",
    "\n",
    "* $y_{ij}$: Latent utility for individual $i$ and alternative $j$.\n",
    "* $\\beta_k$: Parameter $k$. Associated with variable $z_{ijk}$.\n",
    "* $z_{ijk}$: Variable associated with $\\beta_k$ for individual $i$ and alternative $j$.\n",
    "* $r_{ij}$: Individual fixed effect for individual $i$ and alternative $j$.\n",
    "* $\\varepsilon_{ij}$: Random extreme value shock for individual $i$ and alternative $j$.\n",
    "\n",
    "$y_{ij}$ will not be observable in practice, instead we will observe binary outcomes $Y_{ij} \\in \\{0,1\\}$ that reflect a consumer's choice across alternatives. We will have $Y_{ij} = 1$ when $y_{i,j} \\geq y_{i,-j} \\forall -j \\neq j$.\n",
    "\n",
    "## Simulated data to test conditional logit function\n",
    "* Number of alternatives, $J = 5$\n",
    "* Number of observations, $N = 2,000$\n",
    "* Number of parameters, $K = 3$\n",
    "* True parameters values: $[\\beta_1, \\beta_2 ,\\beta_3] = [1, 2, 3] $\n",
    "* $z_{ijk} \\sim N(1, \\sigma_z^2)$ where $\\sigma_z$ is drawn from $2*U[0,1]$\n",
    "* $r_{ij} \\sim N(0, \\sigma_\\varepsilon^2)$ were $\\sigma_r$ is drawn from $U[0,1]$\n",
    "* $\\varepsilon_{ij} \\sim$ Extreme Value Type I. Draws taken by taken random draws from the standard uniform and then transformed: $-log(-log(U[0,1]))$. See: Train 2009, Discrete Choice Methods with Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing if yobs fall in correct range (should be 1): 1\n",
      "(2000, 1) (2000, 15) (2000, 5) (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries \n",
    "import numpy as np\n",
    "\n",
    "# ======= Constants =======\n",
    "\n",
    "# Number of alternatives\n",
    "nalt = 5\n",
    "\n",
    "# Number of observations\n",
    "nobs = 2_000\n",
    "\n",
    "# Number of parameters\n",
    "npar = 3\n",
    "\n",
    "meanz = np.ones((nalt*npar, 1)) # Mean set to 1. \n",
    "sdz = 2 * np.random.uniform(size=(nalt*npar, 1)) # Set a different std for each alt/parameters combo\n",
    "meanr = np.zeros((nalt, 1)) # Mean zero. \n",
    "sdr = np.random.uniform(size=(nalt, 1)) # Set a different standard deviation for each alt\n",
    "\n",
    "# True parameters\n",
    "true_params = np.array([[1.], [2.], [3.]])\n",
    "\n",
    "# Simulations\n",
    "zobs = meanz.T + sdz.T*np.random.normal(size=(nobs, nalt*npar))\n",
    "robs = meanr.T + sdr.T*np.random.normal(size=(nobs, nalt))\n",
    "eps = -np.log(-np.log(np.random.uniform(size=(nobs, nalt))))\n",
    "yobs = np.zeros((nobs, nalt))\n",
    "\n",
    "j = 0\n",
    "while j < nalt:\n",
    "    yobs[:, [j]] = zobs[:, npar*j:npar*(j+1)]@true_params + robs[:, [j]] + eps[:, [j]]\n",
    "\n",
    "    j += 1\n",
    "    \n",
    "yobs = np.argmax(yobs, axis=1)\n",
    "yobs = np.reshape(yobs, (len(yobs), 1))\n",
    "\n",
    "test_elements = np.arange(0, nalt, 1)\n",
    "print(\"Testing if yobs fall in correct range (should be 1): \" + str((np.isin(yobs, test_elements)*1).min()))\n",
    "\n",
    "# Name my parameters \n",
    "namesb = np.array([[\"b1\"], [\"b2\"], [\"b3\"]])\n",
    "\n",
    "print(yobs.shape, zobs.shape, robs.shape, namesb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional logit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clogit(ydum, x, restx, par_names):\n",
    "    \"\"\"\n",
    "    Maximum likelihood estimation of a multinomial logit\n",
    "    _________________________________\n",
    "    \n",
    "    ----Inputs----\n",
    "        \n",
    "    ydum: (N x 1) vector of observations of dependent variable, these will\n",
    "    have values {0, ..., nalt}. I start from 0, unlike AM code. \n",
    "    This is because python lists are 0-indexed.\n",
    "\n",
    "    x: (N x (k * nalt)) matrix of explanatory variables associated with \n",
    "    unrestricted parameters. Firt k columns correspond to alternative 0. \n",
    "    \n",
    "    restx: (N x nalt) vector of the sum of the explanatory variables whose\n",
    "    parameters are restricted to be equal to 1. \n",
    "    \n",
    "    par_names: (k x 1) vector with names of our parameters\n",
    "    \n",
    "    ----Ouputs----\n",
    "    \n",
    "    b0: (k x 1) vector with ML estimates\n",
    "\n",
    "    Avarb: (k x k) matrix with estimate of the covariance matrix \n",
    "    \n",
    "    \"\"\"    \n",
    "    cconvb = 1e-6\n",
    "    myzero = 1e-16\n",
    "    nobs = ydum.shape[0]\n",
    "    nalt = int(np.max(ydum) + 1)\n",
    "    npar = int(x.shape[1]/nalt)\n",
    "    \n",
    "    # Error check \n",
    "    if npar != par_names.shape[0]:\n",
    "        return \"Error: The dimensions of x and names of parameters do not match\"\n",
    "\n",
    "    xysum = 0\n",
    "    \n",
    "    j = 0\n",
    "    while j < nalt:\n",
    "        xysum = xysum + (((ydum==j)*1)*x[:, npar*j:npar*(j + 1)]).sum(axis = 0)\n",
    "        j += 1\n",
    "    \n",
    "    xysum = np.expand_dims(xysum, axis = 1)\n",
    "    \n",
    "    iter = 2  # Number of iterations\n",
    "    criter = 1000\n",
    "    llike = -nobs\n",
    "    b0 = np.ones((npar, 1))\n",
    "    \n",
    "    while criter > cconvb:\n",
    "        \n",
    "        #-----Computing probabilities-----#\n",
    "        phat = np.zeros((nobs, nalt))\n",
    "        \n",
    "        j = 0\n",
    "        while j < nalt:\n",
    "            phat[:, [j]] = x[:, npar*j:npar*(j + 1)]@b0 + restx[:, [j]]\n",
    "            j += 1\n",
    "        \n",
    "        phat_maxs = np.max(phat.T, axis=0)\n",
    "        phat_maxs = np.expand_dims(phat_maxs, axis=1) \n",
    "\n",
    "        phat = phat - phat_maxs\n",
    "        \n",
    "        sumexp = np.exp(phat.T).sum(axis=0)\n",
    "        sumexp = np.expand_dims(sumexp, axis=1)\n",
    "        \n",
    "        phat = np.exp(phat)/sumexp\n",
    "        \n",
    "        #-----Computing xmean-----#\n",
    "        sumpx = np.zeros((nobs, 1))\n",
    "        xxm = 0\n",
    "        llike = 0 # llike is the Log-likelihood function\n",
    "        \n",
    "        j = 0\n",
    "        while j < nalt:\n",
    "            xbuff = x[:, npar*j: npar*(j + 1)]\n",
    "            sumpx = sumpx + phat[:, [j]]*xbuff\n",
    "            xxm = xxm + (phat[:, [j]]*xbuff).T @ xbuff\n",
    "            llike = (llike + (1*(ydum ==j)*np.log(((phat[:,[j]] > myzero)*1)*phat[:,[j]]\n",
    "                                     + ((phat[:, [j]] < myzero)*1)*myzero).sum(axis=0)))\n",
    "            j += 1\n",
    "\n",
    "        #-----Computing gradient-----#\n",
    "        d1llike = xysum - sumpx.sum(axis=0, keepdims=True).T\n",
    "        \n",
    "        #-----Computing Hessian-----#\n",
    "        d2llike = -(xxm - sumpx.T @ sumpx) # \n",
    "        \n",
    "        #-----Gauss iteration-----#\n",
    "        \n",
    "        b1 = b0 - np.linalg.inv(d2llike) @ d1llike\n",
    "        criter = np.sqrt((b1 - b0).T @ (b1 - b0)) \n",
    "        b0 = b1\n",
    "        iter = iter + 1\n",
    "        \n",
    "        Avarb = np.linalg.inv(-d2llike)\n",
    "        \n",
    "        sdb = np.sqrt(np.diag(Avarb))\n",
    "        sdb = np.expand_dims(sdb, axis = 1)\n",
    "        \n",
    "        tstat = b0/sdb\n",
    "        \n",
    "        y_alts = np.arange(0, nalt, 1)\n",
    "        y_alts = np.expand_dims(y_alts, axis=1)\n",
    "        \n",
    "        numyj = ((ydum == y_alts.T)*1).sum(axis=0) # Shape of ydum is important to review\n",
    "        logL0 = (numyj*np.log(numyj/nobs)).sum(axis=0)\n",
    "        logL0 = np.expand_dims(logL0, axis = 0)\n",
    "        \n",
    "        lrindex = 1 - llike/logL0 # Likelihood ratio index\n",
    "        \n",
    "        #-----Prints-----#\n",
    "        print(\"Iteration: \"+str(iter))\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"Parameter  estimate   Standard Errors   t-ratios\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        \n",
    "        j = 0\n",
    "        while j < npar:\n",
    "            print(par_names[j], b0[j], sdb[j], tstat[j])\n",
    "            j += 1\n",
    "        print(\"     \")\n",
    "        \n",
    "    return b0, Avarb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "\n",
    "Estimation of the parameters using the simulated data is presented below. One can see that the estimates approach the true parameters as we take further iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3\n",
      "--------------------------------------------------\n",
      "Parameter  estimate   Standard Errors   t-ratios\n",
      "--------------------------------------------------\n",
      "['b1'] [0.71851404] [0.03216837] [22.33604128]\n",
      "['b2'] [1.34519691] [0.04190127] [32.10396235]\n",
      "['b3'] [1.86787711] [0.03724035] [50.15735229]\n",
      "     \n",
      "Iteration: 4\n",
      "--------------------------------------------------\n",
      "Parameter  estimate   Standard Errors   t-ratios\n",
      "--------------------------------------------------\n",
      "['b1'] [0.8916201] [0.03137732] [28.41606724]\n",
      "['b2'] [1.81496373] [0.05042722] [35.99174585]\n",
      "['b3'] [2.62094486] [0.05613745] [46.68798926]\n",
      "     \n",
      "Iteration: 5\n",
      "--------------------------------------------------\n",
      "Parameter  estimate   Standard Errors   t-ratios\n",
      "--------------------------------------------------\n",
      "['b1'] [1.02321413] [0.03818854] [26.79374716]\n",
      "['b2'] [2.12676722] [0.06571406] [32.36396214]\n",
      "['b3'] [3.09949095] [0.08031679] [38.59082289]\n",
      "     \n",
      "Iteration: 6\n",
      "--------------------------------------------------\n",
      "Parameter  estimate   Standard Errors   t-ratios\n",
      "--------------------------------------------------\n",
      "['b1'] [1.06318155] [0.04342412] [24.48366521]\n",
      "['b2'] [2.21602322] [0.07735767] [28.64645711]\n",
      "['b3'] [3.2328775] [0.09813017] [32.94478628]\n",
      "     \n",
      "Iteration: 7\n",
      "--------------------------------------------------\n",
      "Parameter  estimate   Standard Errors   t-ratios\n",
      "--------------------------------------------------\n",
      "['b1'] [1.06566177] [0.04503976] [23.66046797]\n",
      "['b2'] [2.22140254] [0.08090982] [27.45528985]\n",
      "['b3'] [3.24076933] [0.10344688] [31.32785805]\n",
      "     \n",
      "Iteration: 8\n",
      "--------------------------------------------------\n",
      "Parameter  estimate   Standard Errors   t-ratios\n",
      "--------------------------------------------------\n",
      "['b1'] [1.06567005] [0.04513924] [23.60850746]\n",
      "['b2'] [2.22142021] [0.08112653] [27.38216827]\n",
      "['b3'] [3.24079493] [0.10376723] [31.23139035]\n",
      "     \n",
      "Iteration: 9\n",
      "--------------------------------------------------\n",
      "Parameter  estimate   Standard Errors   t-ratios\n",
      "--------------------------------------------------\n",
      "['b1'] [1.06567005] [0.04513957] [23.60833571]\n",
      "['b2'] [2.22142021] [0.08112724] [27.38192828]\n",
      "['b3'] [3.24079493] [0.10376828] [31.2310764]\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "b0, Avarb = clogit(yobs, zobs, robs, namesb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
